<<<<<<< HEAD
# e4040-2020Fall-project
Seed repo for projects for e4040-2020Fall-project
  - distributed as Github Repo and shared via Github Classroom
  - contains only README.md file
  - Students must have at least one main Jupyter Notebook, and a number of python files in a number of directories and subdirectories such as utils or similar, as demonstrated in the assignments
  - The organization of the directories has to be meaningful

# Detailed instructions how to submit this assignment/homework/project:
1. The assignment will be distributed as a github classroom assignment - as a special repository accessed through a link
2. A students copy of the assignment gets created automatically with a special name - students have to rename the repo per instructions below
3. The solution(s) to the assignment have to be submitted inside that repository as a set of "solved" Jupyter Notebooks, and several modified python files which reside in directories/subdirectories
4. Three files/screenshots need to be uploaded into the directory "figures" which prove that the assignment has been done in the cloud

## (Re)naming of a project repository shared by multiple students (TODO students)
INSTRUCTIONS for naming the students' solution repository for assignments with more students, such as the final project. Students need to use a 4-letter groupID): 
* Template: e4040-2020Fall-Project-GroupID-UNI1-UNI2-UNI3. -> Example: e4040-2020Fall-Project-MEME-zz9999-aa9999-aa0000.

# Organization of this directory
To be populated by students, as shown in previous assignments
=======
# ECBM4040 Final Project: Composing Music With Recurrent Neural Networks

![title page](https://github.com/MorganWenjunYang/ECBM4040-Final-Project/blob/candidate/image/title%20pic.png)

### Author: 
Wenjun Yang (wy2347) || Qihang Yang(qy2231)


# Project Description:
Our project aims at reconstructing the architecture of the Bi-axial LSTM model, and composing music based on the model. The code of the original paper is written in theano,  while we managed to understand the practical implementation and build our model in tensorflow and keras according to the description in the original paper. After epochs of training, we manage to have some satisfying results. Our music now has a sense of chords.


# How to see the result and better leverage our model
To see a thorough workthrough of our project, please go to Report.ipynb where we provide detailed explaination of our idea, model and implementation. 

Due to limitation of storage space of github, unfortunately, the model file can't be pushed into github, however you may find the saved model file in following link. You can build a model and load the weight, so that you can immediately generate music yourself.

All the function we defined are located in utils folder, and organized into 3 scripts, prep.py for all functions we need in data preparation, model.py for the model and custom loss function we use and visualizeMIDI.py for music visualization.

Link to saved model:
https://drive.google.com/file/d/1SFWloQ0ukVv9Lr7ieN98HmfeFsEEWxJz/view?usp=sharing

Note: Download it and save it back to the folder specified above, then you can easily proceed to the next step. 
We are opening view access to everyone with lionmail, if you have trouble downloading that, please feel free to contact us.

# Organization of this directory
>>>>>>> a0c50984e08c88d699f25fc6b60f2d3ec953a78a
